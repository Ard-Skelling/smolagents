{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -e .[litellm] datasets sympy numpy matplotlib seaborn -q  # Install dev version of smolagents + some packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants and utilities/tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark date\n",
    "# - set a concrete date:\n",
    "DATE = \"2024-12-26\"\n",
    "# - or use default: today\n",
    "# DATE = None\n",
    "\n",
    "# Evaluation dataset\n",
    "EVAL_DATASET = \"smolagents-benchmark/benchmark-v1\"\n",
    "\n",
    "# Answers dataset: it must be a gated dataset; required to score the answers\n",
    "ANSWERS_DATASET = \"smolagents-benchmark/answers\"\n",
    "# Whether to push the answers dataset to the Hub\n",
    "PUSH_ANSWERS_DATASET_TO_HUB = True\n",
    "\n",
    "# Results dataset\n",
    "RESULTS_DATASET = \"smolagents-benchmark/results\"\n",
    "# Whether to push the results dataset to the Hub\n",
    "PUSH_RESULTS_DATASET_TO_HUB = True\n",
    "\n",
    "\n",
    "import datetime\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import time\n",
    "import warnings\n",
    "from typing import List\n",
    "\n",
    "import datasets\n",
    "from dotenv import load_dotenv\n",
    "from tqdm import tqdm\n",
    "\n",
    "from smolagents import (\n",
    "    AgentError,\n",
    "    CodeAgent,\n",
    "    GoogleSearchTool,\n",
    "    HfApiModel,\n",
    "    PythonInterpreterTool,\n",
    "    ToolCallingAgent,\n",
    "    VisitWebpageTool,\n",
    "    LiteLLMModel,\n",
    ")\n",
    "from smolagents.agents import ActionStep\n",
    "\n",
    "load_dotenv()\n",
    "os.makedirs(\"output\", exist_ok=True)\n",
    "\n",
    "os.environ[\"SERPAPI_API_KEY\"] = \"a1640c5cf4c085e7c66476a35802465109e7fd3f945052d51d08df8fc4412056\"\n",
    "\n",
    "\n",
    "def serialize_agent_error(obj):\n",
    "    if isinstance(obj, AgentError):\n",
    "        return {\"error_type\": obj.__class__.__name__, \"message\": obj.message}\n",
    "    else:\n",
    "        return str(obj)\n",
    "\n",
    "\n",
    "def answer_questions(\n",
    "    eval_ds,\n",
    "    agent,\n",
    "    model_id,\n",
    "    action_type,\n",
    "    is_vanilla_llm=False,\n",
    "    date=DATE,\n",
    "    output_dir=\"output\",\n",
    "    push_to_hub_dataset=ANSWERS_DATASET if PUSH_ANSWERS_DATASET_TO_HUB else None,\n",
    "):\n",
    "    date = date or datetime.date.today().isoformat()\n",
    "\n",
    "    for task in eval_ds:\n",
    "        file_name = f\"output/{model_id.replace('/', '__')}__{action_type}__{task}__{date}.jsonl\"\n",
    "        answered_questions = []\n",
    "        if os.path.exists(file_name):\n",
    "            with open(file_name, \"r\") as f:\n",
    "                for line in f:\n",
    "                    answered_questions.append(json.loads(line)[\"question\"])\n",
    "\n",
    "        for _, example in tqdm(enumerate(eval_ds[task]), total=len(eval_ds[task])):\n",
    "            try:\n",
    "                question = example[\"question\"]\n",
    "                if example[\"source\"] == \"SimpleQA\":\n",
    "                    question += \" Answer with only the final number.\"\n",
    "                if example[\"source\"] == \"MATH\":\n",
    "                    question += \" Write code, not latex.\"\n",
    "                if question in answered_questions:\n",
    "                    continue\n",
    "                start_time = time.time()\n",
    "\n",
    "                if is_vanilla_llm:\n",
    "                    llm = agent\n",
    "                    answer = str(llm([{\"role\": \"user\", \"content\": question}]).content)\n",
    "                    token_count = {\n",
    "                        \"input\": llm.last_input_token_count,\n",
    "                        \"output\": llm.last_output_token_count,\n",
    "                    }\n",
    "                    intermediate_steps = str([])\n",
    "                else:\n",
    "                    answer = str(agent.run(question))\n",
    "                    token_count = agent.monitor.get_total_token_counts()\n",
    "                    intermediate_steps = str(agent.logs)\n",
    "                    # Remove memory from logs to make them more compact.\n",
    "                    for step in agent.logs:\n",
    "                        if isinstance(step, ActionStep):\n",
    "                            step.agent_memory = None\n",
    "\n",
    "                end_time = time.time()\n",
    "                annotated_example = {\n",
    "                    \"model_id\": model_id,\n",
    "                    \"agent_action_type\": action_type,\n",
    "                    \"question\": question,\n",
    "                    \"answer\": answer,\n",
    "                    \"true_answer\": example[\"true_answer\"],\n",
    "                    \"source\": example[\"source\"],\n",
    "                    \"intermediate_steps\": intermediate_steps,\n",
    "                    \"start_time\": start_time,\n",
    "                    \"end_time\": end_time,\n",
    "                    \"token_counts\": token_count,\n",
    "                }\n",
    "\n",
    "                with open(file_name, \"a\") as f:\n",
    "                    json.dump(annotated_example, f, default=serialize_agent_error)\n",
    "                    f.write(\"\\n\")  # add a newline for JSONL format\n",
    "            except Exception as e:\n",
    "                print(\"Failed:\", e)\n",
    "\n",
    "        if push_to_hub_dataset:\n",
    "            ds = datasets.Dataset.from_pandas(pd.read_json(file_name, lines=True), split=\"test\", preserve_index=False)\n",
    "            config = f\"{model_id.replace('/', '__')}__{action_type}__{task}\"\n",
    "            data_dir = f\"{model_id}/{action_type}/{task}/{date}\"\n",
    "            ds.push_to_hub(\n",
    "                push_to_hub_dataset,\n",
    "                config_name=config,\n",
    "                data_dir=data_dir,\n",
    "                split=\"test\",\n",
    "                commit_message=f\"Upload {config}\",\n",
    "            )\n",
    "\n",
    "\n",
    "def normalize_number_str(number_str: str) -> float:\n",
    "    # we replace these common units and commas to allow\n",
    "    # conversion to float\n",
    "    for char in [\"$\", \"%\", \",\"]:\n",
    "        number_str = number_str.replace(char, \"\")\n",
    "    try:\n",
    "        return float(number_str)\n",
    "    except ValueError:\n",
    "        return float(\"inf\")\n",
    "\n",
    "\n",
    "def split_string(\n",
    "    s: str,\n",
    "    char_list: list[str] = [\",\", \";\"],\n",
    ") -> list[str]:\n",
    "    pattern = f\"[{''.join(char_list)}]\"\n",
    "    return re.split(pattern, s)\n",
    "\n",
    "\n",
    "def is_float(element: any) -> bool:\n",
    "    try:\n",
    "        float(element)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "\n",
    "def normalize_str(input_str, remove_punct=True) -> str:\n",
    "    \"\"\"\n",
    "    Normalize a string by:\n",
    "    - Removing all white spaces\n",
    "    - Optionally removing punctuation (if remove_punct is True)\n",
    "    - Converting to lowercase\n",
    "    Parameters:\n",
    "    - input_str: str, the string to normalize\n",
    "    - remove_punct: bool, whether to remove punctuation (default: True)\n",
    "    Returns:\n",
    "    - str, the normalized string\n",
    "    \"\"\"\n",
    "    # Remove all white spaces. Required e.g for seagull vs. sea gull\n",
    "    no_spaces = re.sub(r\"\\s\", \"\", input_str)\n",
    "\n",
    "    # Remove punctuation, if specified.\n",
    "    if remove_punct:\n",
    "        translator = str.maketrans(\"\", \"\", string.punctuation)\n",
    "        return no_spaces.lower().translate(translator)\n",
    "    else:\n",
    "        return no_spaces.lower()\n",
    "\n",
    "\n",
    "def extract_numbers(text: str) -> List[str]:\n",
    "    \"\"\"This pattern matches:\n",
    "    - Optional negative sign\n",
    "    - Numbers with optional comma thousand separators\n",
    "    - Optional decimal points with decimal numbers\n",
    "    \"\"\"\n",
    "    pattern = r\"-?(?:\\d{1,3}(?:,\\d{3})+|\\d+)(?:\\.\\d+)?\"\n",
    "\n",
    "    return [el.replace(\",\", \"\") for el in re.findall(pattern, text)]\n",
    "\n",
    "\n",
    "def get_question_score_gaia(\n",
    "    model_answer: str,\n",
    "    ground_truth: str,\n",
    ") -> bool:\n",
    "    \"\"\"Scoring function used to score functions from the GAIA benchmark\"\"\"\n",
    "    if is_float(ground_truth):\n",
    "        normalized_answer = normalize_number_str(str(model_answer))\n",
    "        return normalized_answer == float(ground_truth)\n",
    "\n",
    "    elif any(char in ground_truth for char in [\",\", \";\"]):  # if gt is a list\n",
    "        # question with the fish: normalization removes punct\n",
    "        gt_elems = split_string(ground_truth)\n",
    "        ma_elems = split_string(model_answer)\n",
    "\n",
    "        if len(gt_elems) != len(ma_elems):  # check length is the same\n",
    "            warnings.warn(\"Answer lists have different lengths, returning False.\", UserWarning)\n",
    "            return False\n",
    "\n",
    "        comparisons = []\n",
    "        for ma_elem, gt_elem in zip(ma_elems, gt_elems):  # compare each element as float or str\n",
    "            if is_float(gt_elem):\n",
    "                normalized_ma_elem = normalize_number_str(ma_elem)\n",
    "                comparisons.append(normalized_ma_elem == float(gt_elem))\n",
    "            else:\n",
    "                # we do not remove punct since comparisons can include punct\n",
    "                comparisons.append(\n",
    "                    normalize_str(ma_elem, remove_punct=False) == normalize_str(gt_elem, remove_punct=False)\n",
    "                )\n",
    "        return all(comparisons)\n",
    "\n",
    "    else:  # if gt is a str\n",
    "        return normalize_str(model_answer) == normalize_str(ground_truth)\n",
    "\n",
    "\n",
    "def get_correct(row):\n",
    "    if row[\"source\"] == \"MATH\":  # Checks the last number in answer\n",
    "        numbers_answer = extract_numbers(str(row[\"answer\"]))\n",
    "        if len(numbers_answer) == 0:\n",
    "            return False\n",
    "        return float(numbers_answer[-1]) == float(row[\"true_answer\"])\n",
    "    else:\n",
    "        return get_question_score_gaia(str(row[\"answer\"]), str(row[\"true_answer\"]))\n",
    "\n",
    "\n",
    "def score_answers(\n",
    "    answers_subsets,\n",
    "    answers_dataset=ANSWERS_DATASET,\n",
    "    date=DATE,\n",
    "    push_to_hub_dataset=RESULTS_DATASET if PUSH_RESULTS_DATASET_TO_HUB else None,\n",
    "    set_default=True,\n",
    "):\n",
    "    if not answers_dataset:\n",
    "        raise ValueError(\"Pass 'answers_dataset' to load the answers from it\")\n",
    "    date = date or datetime.date.today().isoformat()\n",
    "    results = []\n",
    "    for answers_subset in answers_subsets:\n",
    "        *model_id, action_type, task = answers_subset.split(\"__\")\n",
    "        model_id = \"/\".join(model_id)\n",
    "        ds = datasets.load_dataset(answers_dataset, answers_subset, split=\"test\")\n",
    "        df = ds.to_pandas()\n",
    "        df[\"correct\"] = df.apply(get_correct, axis=1)\n",
    "        acc = df[\"correct\"].mean().item()\n",
    "        result = df.loc[0, [\"model_id\", \"agent_action_type\", \"source\"]].to_dict()\n",
    "        result[\"acc\"] = acc\n",
    "        results.append(result)\n",
    "    df = pd.DataFrame(results)\n",
    "\n",
    "    if push_to_hub_dataset:\n",
    "        ds = datasets.Dataset.from_pandas(df)\n",
    "        config = date\n",
    "        set_default = set_default\n",
    "        ds.push_to_hub(\n",
    "            push_to_hub_dataset, config_name=config, set_default=set_default, commit_message=f\"Upload {config} results\"\n",
    "        )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gaia', 'math', 'simpleqa']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Choose the tasks to evaluate on:\n",
    "# tasks = [\"gaia\"]\n",
    "# or evaluate on all tasks: [\"gaia\", \"math\", \"simpleqa\"]\n",
    "tasks = datasets.get_dataset_config_names(EVAL_DATASET)\n",
    "print(tasks)\n",
    "\n",
    "\n",
    "eval_ds = {task: datasets.load_dataset(EVAL_DATASET, task, split=\"test\") for task in tasks}\n",
    "eval_ds = {\"gaia\": datasets.load_dataset(EVAL_DATASET, \"gaia\", split=\"test\")}\n",
    "#pd.DataFrame(eval_ds[\"simpleqa\"]).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark agents\n",
    "\n",
    "### Open models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                | 0/32 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">╭──────────────────────────────────────────────────── </span><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">New run</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ────────────────────────────────────────────────────╮</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">If Eliud Kipchoge could maintain his record-making marathon pace indefinitely, how many thousand hours would it</span> <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">take him to run the distance between the Earth and the Moon its closest approach? Please use the minimum </span>       <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">perigee value on the Wikipedia page for the Moon when carrying out your calculation. Round your result to the </span>  <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">nearest 1000 hours and do not use any comma separators if necessary.</span>                                            <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">╰─ LiteLLMModel - hosted_vllm//fsx/anton/deepseek-r1-checkpoint ──────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m╭─\u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[1;38;2;212;183;2mNew run\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╮\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mIf Eliud Kipchoge could maintain his record-making marathon pace indefinitely, how many thousand hours would it\u001b[0m \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mtake him to run the distance between the Earth and the Moon its closest approach? Please use the minimum \u001b[0m       \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mperigee value on the Wikipedia page for the Moon when carrying out your calculation. Round your result to the \u001b[0m  \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mnearest 1000 hours and do not use any comma separators if necessary.\u001b[0m                                            \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m╰─\u001b[0m\u001b[38;2;212;183;2m LiteLLMModel - hosted_vllm//fsx/anton/deepseek-r1-checkpoint \u001b[0m\u001b[38;2;212;183;2m─────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m1\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "open_model_ids = [\n",
    "    \"meta-llama/Llama-3.3-70B-Instruct\",\n",
    "    # \"Qwen/QwQ-32B-Preview\",\n",
    "    \"Qwen/Qwen2.5-72B-Instruct\",\n",
    "    \"Qwen/Qwen2.5-Coder-32B-Instruct\",\n",
    "    \"meta-llama/Llama-3.2-3B-Instruct\",\n",
    "    \"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "    \"mistralai/Mistral-Nemo-Instruct-2407\",\n",
    "    # \"HuggingFaceTB/SmolLM2-1.7B-Instruct\",\n",
    "    # \"meta-llama/Llama-3.1-70B-Instruct\",\n",
    "]\n",
    "\n",
    "\n",
    "model_id = \"fsx/anton/deepseek-r1-checkpoint\"\n",
    "action_type = \"code\"\n",
    "agent = CodeAgent(\n",
    "    tools=[GoogleSearchTool(), VisitWebpageTool()],\n",
    "    model = LiteLLMModel(\n",
    "        model_id=f\"hosted_vllm//{model_id}\",\n",
    "        api_base=\"http://ip-26-0-165-202:8000/v1/\",\n",
    "    ),\n",
    "    additional_authorized_imports=[\"numpy\", \"sympy\"],\n",
    "    max_steps=1,\n",
    "    verbosity_level=2,\n",
    ")\n",
    "answer_questions(eval_ds, agent, model_id, action_type)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Also evaluate vanilla model\n",
    "action_type = \"vanilla\"\n",
    "llm = HfApiModel(model_id)\n",
    "answer_questions(eval_ds, llm, model_id, action_type, is_vanilla_llm=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Closed models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from smolagents import LiteLLMModel\n",
    "\n",
    "\n",
    "litellm_model_ids = [\"gpt-4o\", \"anthropic/claude-3-5-sonnet-latest\"]\n",
    "\n",
    "\n",
    "for model_id in litellm_model_ids:\n",
    "    print(f\"Evaluating '{model_id}'...\")\n",
    "    action_type = \"tool-calling\"\n",
    "    agent = ToolCallingAgent(\n",
    "        tools=[\n",
    "            GoogleSearchTool(),\n",
    "            VisitWebpageTool(),\n",
    "            PythonInterpreterTool([\"numpy\", \"sympy\"]),\n",
    "        ],\n",
    "        model=LiteLLMModel(model_id),\n",
    "        max_steps=10,\n",
    "    )\n",
    "    answer_questions(eval_ds, agent, model_id, action_type)\n",
    "\n",
    "    action_type = \"code\"\n",
    "    agent = CodeAgent(\n",
    "        tools=[GoogleSearchTool(), VisitWebpageTool()],\n",
    "        model=LiteLLMModel(model_id),\n",
    "        additional_authorized_imports=[\"numpy\", \"sympy\"],\n",
    "        max_steps=10,\n",
    "    )\n",
    "    answer_questions(eval_ds, agent, model_id, action_type)\n",
    "\n",
    "    # Also evaluate vanilla model\n",
    "    action_type = \"vanilla\"\n",
    "    llm = LiteLLMModel(model_id)\n",
    "    answer_questions(eval_ds, llm, model_id, action_type, is_vanilla_llm=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import glob\n",
    "# import json\n",
    "\n",
    "# jsonl_files = glob.glob(f\"output/*.jsonl\")\n",
    "\n",
    "# for file_path in jsonl_files:\n",
    "#     if \"-Nemo-\" in file_path and \"-vanilla-\" in file_path:\n",
    "#         print(file_path)\n",
    "#         # Read all lines and filter out SimpleQA sources\n",
    "#         filtered_lines = []\n",
    "#         removed = 0\n",
    "#         with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "#             for line in f:\n",
    "#                 try:\n",
    "#                     data = json.loads(line.strip())\n",
    "#                     data[\"answer\"] = data[\"answer\"][\"content\"]\n",
    "#                     # if not any([question in data[\"question\"] for question in eval_ds[\"question\"]]):\n",
    "#                     #     removed +=1\n",
    "#                     # else:\n",
    "#                     filtered_lines.append(json.dumps(data) + \"\\n\")\n",
    "#                 except json.JSONDecodeError:\n",
    "#                     print(\"Invalid line:\", line)\n",
    "#                     continue  # Skip invalid JSON lines\n",
    "#         print(f\"Removed {removed} lines.\")\n",
    "#         # Write filtered content back to the same file\n",
    "#         with open(\n",
    "#             str(file_path).replace(\"-vanilla-\", \"-vanilla2-\"), \"w\", encoding=\"utf-8\"\n",
    "#         ) as f:\n",
    "#             f.writelines(filtered_lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Choose the answers subsets to score:\n",
    "# answers_subsets = [\"meta-llama__Llama-3.1-8B-Instruct__code__gaia\"]\n",
    "# or get all the answers subsets present in the ANSWERS_DATASET\n",
    "answers_subsets = datasets.get_dataset_config_names(ANSWERS_DATASET)\n",
    "print(\"Number of answers_subsets\", len(answers_subsets))\n",
    "print(\"Example of answers_subset\", answers_subsets[0])\n",
    "\n",
    "\n",
    "result_df = score_answers(answers_subsets)\n",
    "result_df[\"acc\"] = (result_df[\"acc\"] * 100).round(2)\n",
    "result_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_df = result_df.pivot_table(\n",
    "    index=[\"model_id\", \"source\"],\n",
    "    columns=[\"action_type\"],\n",
    "    values=\"correct\",\n",
    "    fill_value=float(\"nan\"),\n",
    ").reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(pivot_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib.legend_handler import HandlerTuple  # Added import\n",
    "\n",
    "\n",
    "# Assuming pivot_df is your original dataframe\n",
    "models = pivot_df[\"model_id\"].unique()\n",
    "sources = pivot_df[\"source\"].unique()\n",
    "\n",
    "# Create figure and axis\n",
    "plt.style.use(\"seaborn-v0_8-white\")\n",
    "fig, ax = plt.subplots(figsize=(15, 6))\n",
    "\n",
    "# Set the width of each bar group and positions of the bars\n",
    "width = 0.15  # width of each bar\n",
    "spacing = 0.02  # space between bars within a group\n",
    "group_spacing = 0.2  # space between model groups\n",
    "\n",
    "# Calculate positions for the bars\n",
    "num_sources = len(sources)\n",
    "total_width_per_group = (width + spacing) * num_sources * 2  # *2 for agent and vanilla\n",
    "x = np.arange(len(models)) * (total_width_per_group + group_spacing)\n",
    "\n",
    "# Plot bars for each source\n",
    "for i, source in enumerate(sources):\n",
    "    source_data = pivot_df[pivot_df[\"source\"] == source]\n",
    "    agent_scores = [\n",
    "        source_data[source_data[\"model_id\"] == model][\"code\"].values[0]\n",
    "        if len(source_data[source_data[\"model_id\"] == model]) > 0\n",
    "        else np.nan\n",
    "        for model in models\n",
    "    ]\n",
    "    vanilla_scores = [\n",
    "        source_data[source_data[\"model_id\"] == model][\"vanilla\"].values[0]\n",
    "        if len(source_data[source_data[\"model_id\"] == model]) > 0\n",
    "        else np.nan\n",
    "        for model in models\n",
    "    ]\n",
    "\n",
    "    # Position calculation for each pair of bars\n",
    "    pos = x + i * (width * 2 + spacing)\n",
    "\n",
    "    agent_bars = ax.bar(pos, agent_scores, width, label=f\"{source} (Agent)\", alpha=0.8)\n",
    "    vanilla_bars = ax.bar(\n",
    "        pos + width * 0.6,\n",
    "        vanilla_scores,\n",
    "        width,\n",
    "        hatch=\"////\",\n",
    "        alpha=0.5,\n",
    "        hatch_linewidth=2,\n",
    "        label=f\"{source} (Vanilla)\",\n",
    "        color=\"white\",\n",
    "        edgecolor=agent_bars[0].get_facecolor(),\n",
    "    )\n",
    "\n",
    "# Customize the plot\n",
    "ax.set_ylabel(\"Score\")\n",
    "ax.set_title(\"Model Performance Comparison\")\n",
    "\n",
    "# Set x-axis ticks in the middle of each group\n",
    "group_centers = x + (total_width_per_group - spacing) / 2\n",
    "ax.set_xticks(group_centers)\n",
    "\n",
    "# Wrap long model names to prevent overlap\n",
    "wrapped_labels = [\"\\n\".join(model.split(\"/\")) for model in models]\n",
    "ax.set_xticklabels(wrapped_labels, rotation=0, ha=\"center\")\n",
    "\n",
    "# Modify legend to combine agent and vanilla entries\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "unique_sources = sources\n",
    "legend_elements = [\n",
    "    (handles[i * 2], handles[i * 2 + 1], labels[i * 2].replace(\" (Agent)\", \"\")) for i in range(len(unique_sources))\n",
    "]\n",
    "custom_legend = ax.legend(\n",
    "    [(agent_handle, vanilla_handle) for agent_handle, vanilla_handle, _ in legend_elements],\n",
    "    [label for _, _, label in legend_elements],\n",
    "    handler_map={tuple: HandlerTuple(ndivide=None)},\n",
    "    bbox_to_anchor=(1.05, 1),\n",
    "    loc=\"upper left\",\n",
    ")\n",
    "\n",
    "ax.yaxis.grid(True, linestyle=\"--\", alpha=0.3)\n",
    "ax.set_ylim(bottom=0)\n",
    "plt.tight_layout()\n",
    "ax.spines[\"top\"].set_visible(False)\n",
    "ax.spines[\"right\"].set_visible(False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mathjax_table(pivot_df, formatted_df):\n",
    "    # Start the matrix environment with 4 columns\n",
    "    # l for left-aligned model and task, c for centered numbers\n",
    "    mathjax_table = \"\\\\begin{array}{llcc}\\n\"\n",
    "    mathjax_table += \"\\\\text{Model} & \\\\text{Task} & \\\\text{Agent} & \\\\text{Vanilla} \\\\\\\\\\n\"\n",
    "    mathjax_table += \"\\\\hline\\n\"\n",
    "\n",
    "    # Sort the DataFrame by model_id and source\n",
    "    formatted_df = formatted_df.sort_values([\"model_id\", \"source\"])\n",
    "\n",
    "    current_model = None\n",
    "    for _, row in formatted_df.iterrows():\n",
    "        model = row[\"model_id\"]\n",
    "        source = row[\"source\"]\n",
    "\n",
    "        # Add a horizontal line between different models\n",
    "        if current_model is not None and current_model != model:\n",
    "            mathjax_table += \"\\\\hline\\n\"\n",
    "\n",
    "        # Format model name\n",
    "        model_display = model.replace(\"_\", \"\\\\_\")\n",
    "        if \"Qwen\" in model or \"anthropic\" in model:\n",
    "            model_display = f\"\\\\textit{{{model_display}}}\"\n",
    "\n",
    "        # If it's the same model as previous row, use empty space\n",
    "        if current_model == model:\n",
    "            model_display = \"\\\\;\"\n",
    "\n",
    "        # Add the data row\n",
    "        mathjax_table += f\"{model_display} & {source} & {row['agent']} & {row['vanilla']} \\\\\\\\\\n\"\n",
    "\n",
    "        current_model = model\n",
    "\n",
    "    mathjax_table += \"\\\\hline\\n\"\n",
    "    mathjax_table += \"\\\\end{array}\"\n",
    "\n",
    "    return mathjax_table\n",
    "\n",
    "\n",
    "# Usage (after running your previous data processing code):\n",
    "# mathjax_table = create_mathjax_table(pivot_df, formatted_df)\n",
    "# print(mathjax_table)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "smolagents",
   "language": "python",
   "name": "smolagents"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
